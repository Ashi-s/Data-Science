{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import unicodecsv as csv\n",
    "import unicodedata as un\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train folder contains thoushands of sub files with each file representing a movie review. \n",
    "# Merging all the files together and converting them with a single CSV file \n",
    "# repeat the same process for the Test folder.\n",
    "in_path = './aclImdb_v1/aclImdb/train/'\n",
    "out_path= './aclImdb_v1/aclImdb/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files processed:  25001\n",
      "Number of files error:  0\n",
      "Time taken in seconds: 96.64269304275513\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "error_counter = 0\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "label = 0\n",
    "\n",
    "# Prepare dictionary of necessary unicode\n",
    "# Thanks to https://stackoverflow.com/a/11066687/4595807\n",
    "\n",
    "table = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                        if un.category(chr(i)).startswith(('P','N','S','Cf','Cn','Cc')))\n",
    "\n",
    "# Stop words are common words which occurs very frequently like [a, the, in, of etc..]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Storing all the punctuation in the translator variable, so that it can removed at once.\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "\n",
    "with open(out_path, 'wb') as out_file:\n",
    "    writer = csv.writer(out_file, encoding='utf-8')\n",
    "\n",
    "    header = (\"label\", \"data\")\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for root, dirs, files in os.walk(in_path, topdown=True):\n",
    "        for name in files:\n",
    "            curr_file = os.path.join(root, name)\n",
    "            \n",
    "            try:\n",
    "                # Read current file and remove BOM and newline characters\n",
    "                fp = open(curr_file, encoding='utf-8-sig').read()\n",
    "                \n",
    "                # Remove all the HTML tags present in the reviews.(Data Cleaning.)\n",
    "                fp = fp.replace('<br />', '')\n",
    "                \n",
    "                # Remove all punctuations.\n",
    "                fp = fp.translate(table)\n",
    "                \n",
    "                # Remove numbers/symbols/invalid chars\n",
    "                fp = fp.translate(table)\n",
    "                \n",
    "                # Convert everything into Lower Case.\n",
    "                fp = fp.lower()\n",
    "                \n",
    "                # We are not tokenizing the word because we want to read the whole data as a String \n",
    "                # rather then tokens.\n",
    "                \n",
    "                #tokenize the word\n",
    "                #fp = word_tokenize(fp)\n",
    "                \n",
    "                \n",
    "                # We are not removing the stopping words because they make sense in complete sentence and have importance.\n",
    "                #Remove the Stop words.\n",
    "                #fp = [w for w in fp if not w in stop_words]\n",
    "                # print(fp)\n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "                # Normalize the unicode so that\n",
    "                # canonical-equivalent ones will also have precisely the same binary representation\n",
    "                final_msg = label, fp\n",
    "                \n",
    "                # Write into CSV file format - label, data\n",
    "                writer.writerow(final_msg)\n",
    "\n",
    "                # Counter setup to count file processed\n",
    "                counter = counter + 1\n",
    "#                 break\n",
    "                \n",
    "\n",
    "            except IOError as e:\n",
    "                print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "                error_counter = error_counter + 1\n",
    "#                 break\n",
    "                \n",
    "                \n",
    "        label += 1\n",
    "\n",
    "#Closing the File         \n",
    "out_file.close()\n",
    "\n",
    "# Storing the end time for pre-processing.\n",
    "end_time = time.time()\n",
    "\n",
    "print('Number of files processed: ',counter)\n",
    "print('Number of files error: ',error_counter)\n",
    "\n",
    "print('Time taken in seconds:',(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
