{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Ashish Kumar\n",
    "        - Word Embedding (Word2Vec)\n",
    "        - Deep Learning\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import unicodecsv as csv\n",
    "import unicodedata as un\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    #Steps before performing Word2Vec\n",
    "    \n",
    "    Read all files into one corpus file\n",
    "    Break down each sentence into new line\n",
    "    Remove punctuation except selective punctuation\n",
    "    Lowercase\n",
    "    Replace numbers with <num>\n",
    "    Do not remove stop words\n",
    "    Remove '.'\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = './data/data'\n",
    "out_file = './data/raw_corpus.txt'\n",
    "\n",
    "# Create a corpus and remove html break \n",
    "with open(out_file, 'w', encoding='utf8') as out_f:\n",
    "    \n",
    "    for root, dirs, files in os.walk(in_path, topdown=True):\n",
    "        if(len(files) > 0):\n",
    "            for each_file in files:\n",
    "            \n",
    "                file_path = os.path.join(root, each_file)\n",
    "                #print(file_path)\n",
    "                try:\n",
    "                    content = open(file_path, encoding='utf-8-sig').read().replace('<br />', '')\n",
    "                    out_f.write(content)\n",
    "                except IOError as error:\n",
    "                    print(each_file)\n",
    "                    \n",
    "                \n",
    "\n",
    "out_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = './data/raw_corpus.txt'\n",
    "in_p = open(out_file, encoding='utf-8').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# # https://stackoverflow.com/a/31505798/4595807\n",
    "# import re\n",
    "# alphabets= \"([A-Za-z])\"\n",
    "# numbers = \"([0-9])\"\n",
    "# symbols = \"[,\\\\\\/@#$&%\\-'\\(\\):;=\\+*\\^\\~\\[\\]\\\"¿”’´£]\"\n",
    "# prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "# suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "# starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "# acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "# websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "# def split_into_sentences(text):\n",
    "#     text = \" \" + text + \"  \"\n",
    "#     text = text.replace(\"\\n\",\" \")\n",
    "#     text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "#     text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    \n",
    "#     if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "#     text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "#     text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "#     text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "#     text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "#     text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "#     text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "#     text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "#     text = re.sub(numbers,\"<num> \",text)\n",
    "    \n",
    "\n",
    "#     if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "#     if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "#     if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "#     if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "#     text = text.replace(\".\",\"<stop>\")\n",
    "#     text = text.replace(\"?\",\"<stop>\")\n",
    "#     text = text.replace(\"!\",\"<stop>\")\n",
    "#     text = text.replace(\"<prd>\",\".\")\n",
    "   \n",
    "#     text = text.replace(\"<br />\",\"\")\n",
    "#     text = re.sub(symbols, '', text)\n",
    "#     text = text.lower()\n",
    "# #     text = re.sub('^(x\\.[0-9]+.*?).*$', '', text)\n",
    "#     sentences = text.split(\"<stop>\")\n",
    "#     sentences = sentences[:-1]\n",
    "#     sentences = [s.strip() for s in sentences if len(s) > 1]\n",
    "#     return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alan rickman  emma thompson give good performances with southernnew orleans accents in this detective flick', 'its worth seeing for their scenes and rickmans scene with hal holbrook']\n"
     ]
    }
   ],
   "source": [
    "# sent = split_into_sentences(in_p)\n",
    "# print(sent[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk \n",
    "\n",
    "# total_sent = []\n",
    "# for each in sent:\n",
    "#     total_sent.append(each.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alan', 'rickman', 'emma', 'thompson', 'give', 'good', 'performances', 'with', 'southernnew', 'orleans', 'accents', 'in', 'this', 'detective', 'flick'], ['its', 'worth', 'seeing', 'for', 'their', 'scenes', 'and', 'rickmans', 'scene', 'with', 'hal', 'holbrook']]\n"
     ]
    }
   ],
   "source": [
    "# print(total_sent[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. James Ph.D. someone told me Dr. Brown is not available today.', 'I will try tomorrow.', 'I am a good boy.']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer on a piece of text\n",
    "sentences = \"Mr. James Ph.D. someone told me Dr. Brown is not available today. I will try tomorrow. I am a good boy.\"\n",
    "sentences = tokenizer.tokenize(sentences)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_period(sents):\n",
    "    tokenized_sents = []\n",
    "    for each_sent in sents:\n",
    "        if len(each_sent) > 1 and each_sent[-1] == '.':\n",
    "            tokenized_sents.append(each_sent[:-1])\n",
    "        else:\n",
    "            tokenized_sents.append(each_sent)\n",
    "    \n",
    "    return tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = tokenizer.tokenize(in_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook.\", 'These three actors mannage to entertain us no matter what the movie, it seems.', 'The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been.', 'The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things.', 'The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.I have seen this movie and I did not care for this movie anyhow.', 'I would not think about going to Paris because I do not like this country and its national capital.', 'I do not like to learn french anyhow because I do not understand their language.', 'Why would I go to France when I rather go to Germany or the United Kingdom?', 'Germany and the United Kingdom are the nations I tolerate.']\n"
     ]
    }
   ],
   "source": [
    "print(sent[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = remove_period(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Tokenize each sentence\n",
    "'''\n",
    "import nltk\n",
    "total_sents = []\n",
    "\n",
    "for each in sent:\n",
    "    total_sents.append(each.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's\",\n",
       " 'worth',\n",
       " 'seeing',\n",
       " 'for',\n",
       " 'their',\n",
       " 'scenes-',\n",
       " 'and',\n",
       " \"Rickman's\",\n",
       " 'scene',\n",
       " 'with',\n",
       " 'Hal',\n",
       " 'Holbrook']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"./data/word2vec.model\")\n",
    "model = Word2Vec(total_sent, size=50, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"./data/word2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goodbye', 0.7230677008628845),\n",
       " ('yrold', 0.6914480328559875),\n",
       " ('hi', 0.6875802278518677),\n",
       " ('theatremy', 0.6800764203071594),\n",
       " ('dear', 0.6800364255905151),\n",
       " ('bam', 0.6675757169723511),\n",
       " ('pingoin', 0.6615097522735596),\n",
       " ('monette', 0.6544197201728821),\n",
       " ('sweaterhairy', 0.6518028974533081),\n",
       " ('granddad', 0.6479794979095459)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    lametization/ Stemming\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    lametization/ Stemming\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-78-30d9e09e5d27>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-78-30d9e09e5d27>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    with open('./data/clean_corpus.txt',encoding='utf-8','wb') as f:\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/clean_corpus.txt',encoding='utf-8','wb') as f:\n",
    "   f.write(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
